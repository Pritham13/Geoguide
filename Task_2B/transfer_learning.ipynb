{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 10:32:46.461034: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-18 10:32:46.461112: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-18 10:32:46.461932: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-18 10:32:46.553367: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-18 10:32:47.837285: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Lambda, Dense, Flatten\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import optimizers\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = [244,244,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 10:32:59.098549: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "vgg = VGG16(input_shape=image_size, weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for layer in vgg.layers:\n",
    "  layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_eyrc  mymodel.h5\t test_dataset_eyrc\n",
      "mymodel1.h5   task_2b_evaluator  transfer_learning.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "folders = glob('dataset_eyrc/*')\n",
    "print(len(folders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 244, 244, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 244, 244, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 244, 244, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 122, 122, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 122, 122, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 122, 122, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 61, 61, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 61, 61, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 61, 61, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 61, 61, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 30, 30, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 30, 30, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 30, 30, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 30, 30, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 15, 15, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 15, 15, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 15, 15, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 15, 15, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 125445    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14840133 (56.61 MB)\n",
      "Trainable params: 125445 (490.02 KB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x = Flatten()(vgg.output)\n",
    "prediction = Dense(len(folders), activation='softmax')(x)\n",
    "model = Model(inputs=vgg.input, outputs=prediction)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "adam = optimizers.Adam()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=0,\n",
    "    width_shift_range=0,\n",
    "    height_shift_range=0,\n",
    "    shear_range=0,\n",
    "    zoom_range=0,\n",
    "    horizontal_flip=False,\n",
    "    fill_mode='nearest')\n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=0,\n",
    "    width_shift_range=0,\n",
    "    height_shift_range=0,\n",
    "    shear_range=0,\n",
    "    zoom_range=0,\n",
    "    horizontal_flip=False,\n",
    "    fill_mode='nearest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'dataset_eyrc'\n",
    "test_path = 'test_dataset_eyrc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 images belonging to 5 classes.\n",
      "Found 10 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_set = train_datagen.flow_from_directory(train_path,\n",
    "                                                 target_size = (224, 224),\n",
    "                                                 batch_size = 16,\n",
    "                                                 class_mode = 'categorical')\n",
    "test_set = train_datagen.flow_from_directory(test_path,\n",
    "                                                 target_size = (224, 224),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "25/25 - 29s - loss: 5.1392e-07 - accuracy: 1.0000 - 29s/epoch - 1s/step\n",
      "Epoch 2/5\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "25/25 - 30s - loss: 4.1217e-07 - accuracy: 1.0000 - 30s/epoch - 1s/step\n",
      "Epoch 3/5\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "25/25 - 30s - loss: 3.5946e-07 - accuracy: 1.0000 - 30s/epoch - 1s/step\n",
      "Epoch 4/5\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "25/25 - 29s - loss: 3.1199e-07 - accuracy: 1.0000 - 29s/epoch - 1s/step\n",
      "Epoch 5/5\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "25/25 - 30s - loss: 2.7762e-07 - accuracy: 1.0000 - 30s/epoch - 1s/step\n",
      "Training completed in time:  0:02:27.314895\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(filepath='mymodel1.h5', \n",
    "                               verbose=2, save_best_only=True)\n",
    "\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "model_history=model.fit(\n",
    "  train_set,\n",
    "  validation_data=test_set,\n",
    "  epochs=5,\n",
    "  steps_per_epoch=25,\n",
    "  validation_steps=32,\n",
    "    callbacks=callbacks ,verbose=2)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/zero/eyrc/TASK2B/transfer_learning.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zero/eyrc/TASK2B/transfer_learning.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m _\u001b[39m# Plot training & validation loss values\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/zero/eyrc/TASK2B/transfer_learning.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(model_history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zero/eyrc/TASK2B/transfer_learning.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(model_history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zero/eyrc/TASK2B/transfer_learning.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m'\u001b[39m\u001b[39mCNN Model accuracy values\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_history' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "_# Plot training & validation loss values\n",
    "plt.plot(model_history.history['accuracy'])\n",
    "plt.plot(model_history.history['val_accuracy'])\n",
    "plt.title('CNN Model accuracy values')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 784ms/step\n",
      "[[0.00000000e+00 2.67467828e-38 9.99999940e-01 1.85315968e-33\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 1.15216135e-30 0.00000000e+00 0.00000000e+00\n",
      "  9.99999940e-01]\n",
      " [0.00000000e+00 1.66667662e-34 1.23204024e-30 9.99999940e-01\n",
      "  1.92385626e-29]\n",
      " [8.95908204e-17 6.64233353e-22 6.46958561e-13 9.99999940e-01\n",
      "  4.93747646e-24]\n",
      " [1.10337098e-29 0.00000000e+00 0.00000000e+00 9.99999940e-01\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 9.99999940e-01 1.35082715e-36 1.81331884e-20\n",
      "  7.66048865e-22]\n",
      " [1.23631783e-33 9.99999940e-01 9.11032587e-36 4.21686650e-34\n",
      "  3.29554887e-24]\n",
      " [6.95762798e-22 2.17920206e-27 3.52969605e-29 9.99999940e-01\n",
      "  1.14758305e-28]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('mymodel.h5')\n",
    "predictions = loaded_model.predict(test_set)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fire', 'Military vehicles and weapons', 'Humanitarian Aid and rehabilitation', 'Humanitarian Aid and rehabilitation', 'Humanitarian Aid and rehabilitation', 'DestroyedBuildings', 'DestroyedBuildings', 'Humanitarian Aid and rehabilitation', 'Fire', 'Military vehicles and weapons']\n"
     ]
    }
   ],
   "source": [
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "class_mapping = {0: 'Combat', 1: 'DestroyedBuildings', 2: 'Fire', 3: 'Humanitarian Aid and rehabilitation', 4: 'Military vehicles and weapons'}\n",
    "\n",
    "predicted_labels = [class_mapping[prediction] for prediction in predicted_classes]\n",
    "print(predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fire', 'Military vehicles and weapons', 'Humanitarian Aid and rehabilitation', 'Humanitarian Aid and rehabilitation', 'Humanitarian Aid and rehabilitation', 'DestroyedBuildings', 'DestroyedBuildings', 'Humanitarian Aid and rehabilitation', 'Fire', 'Military vehicles and weapons']\n"
     ]
    }
   ],
   "source": [
    "class_names = ['Combat', 'DestroyedBuildings', 'Fire','Humanitarian Aid and rehabilitation','Military vehicles and weapons']\n",
    "\n",
    "predicted_class_names = [class_names[prediction] for prediction in predicted_classes]\n",
    "print(predicted_class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@23.280] global loadsave.cpp:248 findDecoder imread_('/home/zero/eyrc/dataset_eyrc/Combat/1.jpeg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) /home/conda/feedstock_root/build_artifacts/libopencv_1692668191568/work/modules/imgproc/src/resize.cpp:4062: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m/home/zero/eyrc/TASK2B/transfer_learning.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zero/eyrc/TASK2B/transfer_learning.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(image_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zero/eyrc/TASK2B/transfer_learning.ipynb#X24sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Resize the image to match the expected input shape of your model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/zero/eyrc/TASK2B/transfer_learning.ipynb#X24sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mresize(image, (\u001b[39m244\u001b[39;49m, \u001b[39m244\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zero/eyrc/TASK2B/transfer_learning.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# You may need to further preprocess the image according to your model's requirements.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zero/eyrc/TASK2B/transfer_learning.ipynb#X24sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zero/eyrc/TASK2B/transfer_learning.ipynb#X24sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Make predictions\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zero/eyrc/TASK2B/transfer_learning.ipynb#X24sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m predictions \u001b[39m=\u001b[39m loaded_model\u001b[39m.\u001b[39mpredict(np\u001b[39m.\u001b[39mexpand_dims(image, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.8.0) /home/conda/feedstock_root/build_artifacts/libopencv_1692668191568/work/modules/imgproc/src/resize.cpp:4062: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Define the path to the image\n",
    "\n",
    "image_path = '/home/zero/eyrc/dataset_eyrc/Combat/1.jpeg'\n",
    "\n",
    "# Load your model\n",
    "loaded_model = load_model('mymodel.h5')\n",
    "\n",
    "# Load and preprocess the image\n",
    "image = cv2.imread(image_path)\n",
    "# Resize the image to match the expected input shape of your model\n",
    "image = cv2.resize(image, (244, 244))\n",
    "# You may need to further preprocess the image according to your model's requirements.\n",
    "\n",
    "# Make predictions\n",
    "predictions = loaded_model.predict(np.expand_dims(image, axis=0))\n",
    "\n",
    "# Find the predicted class\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Define a class mapping\n",
    "class_mapping = {0: 'combat', 1: 'destroyedbuilding', 2: 'Fire', 3: 'humanitarianaid', 4: 'militaryvehicles'}\n",
    "\n",
    "# Map the predicted class to its corresponding label\n",
    "event = [class_mapping[prediction] for prediction in predicted_classes]\n",
    "\n",
    "# Print the result\n",
    "print(event)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GG_1289",
   "language": "python",
   "name": "gg_1289"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
